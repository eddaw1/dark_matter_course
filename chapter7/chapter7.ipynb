{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754c6a25",
   "metadata": {},
   "source": [
    "# Noise in axion experiments\n",
    "\n",
    "## Nyquist's theorem for the noise power emitted by a resistor\n",
    "\n",
    "We have an apparatus at some low temperature $T$ with microwaves moving around it, and the microwaves we care most about are carried in cables. Some of those microwaves are potentially originating with axion dark matter. The vast majority of them are instead originating in the random motions of charged electrons undergoing random motion at the physical temperature $T$ of the apparatus. \n",
    "\n",
    "This derivation is all about figuring out the noise power emitted by a humble resistor. In order to do that, we start off by examining a coaxial cable that the resistor will eventually be connected to. Here is a diagram of that cable. \n",
    "\n",
    "![coaxial_cable](./figures/cable1.png)\n",
    "\n",
    "The cable is of length $L$. We're going to focus on travelling waves travelling to the right. These can be represented by an ordinary one dimensional travelling wave, $\\psi=\\psi^0\\cos(kx-\\omega t)$. We set $t=0$ and focus on what things look like in $x$, so $\\psi=\\psi^0\\cos(kx)$. Now we do something that on the face of it seems peculier, and assert some boundary conditions that are not really physical. These are called periodic boundary conditions, and state that only waves whose phase is the same at the two ends of this piece of cable are permitted. This means that the longest wavelength that can travel down the cable is $L$. We'll assume that waves travel at speed $c$ in the cable, so that $\\omega=ck$. For the boundary condition to be true we must have\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\cos(kx+kL)&=\\cos(kx) \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "But using an addition formula,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\cos(kx+kL)&=\\cos(kx)\\cos(kL)-\\sin(kx)\\sin(kL).\n",
    "\\end{align*}$$\n",
    "\n",
    "For this to equal $\\cos(kx)$, we must have $\\cos(kL)=1$ and $\\sin(kL)=0$. These two constraints are only satisfied if $kL=2n\\pi$. This means that with these periodic boundary conditions in place, the gap between modes in wavenumber is $\\delta k\\,L=2\\pi$, or\n",
    "$\\delta k = 2\\pi/L$. \n",
    "\n",
    "Now, we're actually interested in the frequency spacing between modes. Because $\\omega = 2\\pi f=ck$ we have $\\delta k = 2\\pi\\delta f/c$. We substitute that into our expression for $\\delta k$ and obtain\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{2\\pi}{L}&=\\frac{2\\pi \\delta f}{c}. \\\\\n",
    "\\delta f &= \\frac{c}{L}.\n",
    "\\end{align*}$$\n",
    "\n",
    "Now, suppose we have a frequency range between two ends $f_1$ and $f_2$, called a band, with bandwidth $B=f_2-f_1$. Then the number of modes that fit in this band is $B/(c/L)=BL/c$. If each of these modes is fed by some source of radiation at temperature $T$, and has reached thermodynamic equilibrium with this radiation source, then by the classical theorem of equipartition, each mode contains an average energy of $k_BT$. Recall that modes of oscillators each have two degrees of freedom owing to their storing energy in kinetic and potential forms. Therefore the total energy $U$ in the modes in this band is\n",
    "\n",
    "$$\\begin{align*}\n",
    "U&=\\frac{k_BTBL}{c}.\n",
    "\\end{align*}$$\n",
    "\n",
    "The energy density, or energy per unit length in these modes, $u$, is\n",
    "\n",
    "$$\\begin{align*}\n",
    "u=\\frac{U}{L}=\\frac{k_BTB}{c}\n",
    "\\end{align*}$$\n",
    "\n",
    "The average energy exiting out of the right hand end of the cable  per unit time is the energy density in these modes times their velocity of propagation, which is c. This leads to a formula for the power exiting the right hand end of the cable in a bandwidth, or range, $B$, in frequency.\n",
    "\n",
    "$$\\begin{align*}\n",
    "P=k_BTB\n",
    "\\tag{7.1}\n",
    "\\end{align*}$$\n",
    "\n",
    "Now let us terminate the cable at both ends with resistive loads, whose resistances are equal to the characteristic impedence of the cable. The characteristic impedence is the ratio of the voltage between the outer conductor and the current flowing in the conductors (in opposite directions in the two conductors). \n",
    "\n",
    "![terminated_cable](./figures/cable1terminated.png)\n",
    "\n",
    "It turns out that if the characteristic impedence is made equal to the terminating impedences, any power travelling down the cable and encountering the resistor is absorbed by the resistor without any reflection.\n",
    "\n",
    "We are requiring that the cable is in thermodynamic equilibrium with the resistances. If all the power that flows into the resistors is absorbed without reflection, then if the resistor doesn't emit anything it would warm up because of the energy dissipated in it from the incoming power. In thermodynamic equilibrium, it must be that the resistor emits the same amount of power as it is absorbing from the cable in the bandwidth $B$. Since this argument is independent of the bandwidth, we conclude that Equation 7.1 is also a formula for the power emitted into a bandwidth $B$ of frequency by a resistor at temperature $T$ in thermodynamic equilibrium with a cable with characteristic impedence equal to that of the resistor. This result is known as Nyquist's theorem. \n",
    "\n",
    "We can extend this result by imagining that we measure the energy that the resistor emits, or absorbs, in a time $t$. This will be the power times the time interval $t$. On average, the energy in the flowing radiation will be\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overline{E}&=k_BTBt\n",
    "\\end{align*}$$\n",
    "\n",
    "## Fluctuations and the radiometer equation\n",
    "\n",
    "As shown above, we can work out the average energy emitted by a cable, or a resistor matched to it, into a bandwidth $B$ when they have reached thermodynamic equilibrium at temperature $T$, when the energy is measured over a time interval $t$. However, this is only the average energy flow. In practice, in any given time interval $t$, the energy will probably be near to this average value, but will differ from it by some small amount. After all, the radiation being measured consists of noise, and noise is always subject to fluctuations. These fluctuations need to be understood by the experimentalist who is trying to detect a small signal, such as that which may be emitted by axion dark matter converting into radiation in our detector.\n",
    "\n",
    "The level of fluctuations in the noise background is regulated by the radiometer equation, which we now derive. Consider a classical system which has available to it a contiouous set of states having energy $E$. The system is in thermal equilibrium at a temperature $T$. We do not know how many available states there are at each energy $E$, or indeed what the system consists of, but we do know that it is governed by Maxwell Bolzmann statistics. The probability density $p(E)$ of the system having energy $E$ can be \n",
    "\n",
    "$$\\begin{align*}\n",
    "p(E)=A(T)e^{-\\frac{E}{k_BT}},\n",
    "\\end{align*}$$\n",
    "\n",
    "where $A(T)$ is the number of available states at energy $E$, and is generally a function of the temperature of the system. It will make the derivation easier to write $\\beta=1/(k_BT)$, so that we obtain\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(E)=A(\\beta)e^{-\\beta E}.\n",
    "\\end{align*}$$\n",
    "\n",
    "The probability density obeys a normalisation condition, so that\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\int_{E=0}^\\infty p(E)\\,dE=\\int A(\\beta)e^{-\\beta E}\\,dE=1,\n",
    "\\end{align*}$$\n",
    "\n",
    "where during this derivation all integrals will be between $E=0$ and $E=+\\infty$. We differentiate this expression with respect to $\\beta$ and obtain\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{d}{d\\beta}\\int A(\\beta)e^{-\\beta E}\\,dE&=0,\n",
    "\\end{align*}$$\n",
    "\n",
    "We bring the differential inside the integral over energies and apply the product rule to differentiate first $A(\\beta)$ and then $e^{-\\beta E}$, both of which are $\\beta$ dependent.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\int \\left(\\frac{dA}{d\\beta}e^{-\\beta E}-E\\,A(\\beta)e^{-\\beta E}\\right)\\,dE&=0 \\\\\n",
    "\\int \\frac{dA}{d\\beta}e^{-\\beta E}\\,dE&=\\int\\,E\\,A(\\beta)e^{-\\beta E}.\n",
    "\\end{align*}$$\n",
    "\n",
    "The right hand side is $\\int\\,Ep(E)dE$, which is $\\overline{E}$, the average value of the energy. The $dA/d\\beta$ on the left has no $E$ dependence and can be pulled out of the integral. In addition, we can introduce $A/A$ on the right. We obtain\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{1}{A}\\frac{dA}{d\\beta}\\int\\,A\\,e^{-\\beta E}\\,dE &=\\overline{E}. \n",
    "\\end{align*}$$\n",
    "\n",
    "The integral is $1$ because it is equal to $\\int\\,p(E)dE$. Hence we can write\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{dA}{d\\beta}&=A\\overline{E}.\n",
    "\\tag{7.2}\n",
    "\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80333b",
   "metadata": {},
   "source": [
    "We next return to the expression for $\\overline{E}$, the average energy.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overline{E}&=\\int\\,E\\,p(E)\\,dE=\\int E\\,A(\\beta)e^{-\\beta E}\\,dE\n",
    "\\end{align*}$$\n",
    "\n",
    "Differentiating,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{d\\overline{E}}{d\\beta}&=\\int\\frac{dA}{d\\beta}\\,E\\,e^{-\\beta E}\\,dE-\\int\\,A\\,E^2\\,e^{-\\beta E}\\,dE \\\\\n",
    "&=\\int\\frac{dA}{d\\beta}\\,E\\,e^{-\\beta E}\\,dE-\\overline{E^2}.\n",
    "\\end{align*}$$\n",
    "\n",
    "We substitute in from Equation 7.2, and factor the $\\overline{E}$ out of the left hand integral.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{d\\overline{E}}{d\\beta}&=\\overline{E}\\int\\,E\\,A\\,e^{-\\beta E}\\,dE-\\overline{E^2}. \\\\\n",
    "&=\\overline{E}^2-\\overline{E^2}.\n",
    "\\tag{7.3}\n",
    "\\end{align*}$$\n",
    "\n",
    "Next we re-introduce temperature $T$ by writing\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{d\\overline{E}}{d\\beta}&=\\frac{d\\overline{E}}{dT}\\frac{dT}{d\\beta}. \n",
    "\\end{align*}$$\n",
    "\n",
    "Now, $\\beta=1/(k_BT)$, so that $d\\beta/dT=-1/(k_BT^2)$, and $dT/d\\beta=-k_BT^2$. Substituting this in to Equation 7.3, we get\n",
    "\n",
    "$$\\begin{align*}\n",
    "k_B T^2\\frac{d\\overline{E}}{dT}&=\\overline{E^2}-\\overline{E}^2. \n",
    "\\end{align*}$$\n",
    "\n",
    "Now, the difference between the average of the square of a quantity and the square of the average of a quantity is also known as the variance of that quantity. We therefore write $\\sigma_E^2=\\overline{E^2}-\\overline{E}^2$, so that we arrive at\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sigma_E^2&=k_BT^2\\frac{d\\overline{E}}{dT}.\n",
    "\\tag{7.4}\n",
    "\\end{align*}$$\n",
    "\n",
    "This result was first derived, as far as I know, by J. Willard Gibbs in his book Elementary Principles In Statistical Mechanics, published in 1901. Though the title makes it seem like a textbook, in fact this is the book where Gibbs laid out the foundations of statistical mechanics. The derivation is on pages 68-72. Its importance to axion searches will become clear, but it is also of central importance in radio astronomy and any other field where measurements of a signal on top of noise in thermal equilibrium are being made. This is, of course, most experiments, so it is a very important result. I think this result is one of the unsung heroes of physics, because of its immense generality and utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666cdaa1",
   "metadata": {},
   "source": [
    "## The radiometer equation applied to Johnson noise.\n",
    "\n",
    "For Johnson noise, we know that the average power emitted by a resistor at temperature $T$ matched to a transmission line is $P=k_B T B$, where $B$ is the bandwidth. Therefore, in a time interval $t$, the average amount of energy transmitted by the noise source is $\\overline{E}=k_B T B t$. We insert this average energy into Gibb's radiometer equation, 7.4, and obtain\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sigma_E^2&=k_B^2 T^2 B t \\\\\n",
    "\\sigma_E&= k_B T\\sqrt{Bt} \\\\\n",
    "\\frac{\\sigma_E}{\\overline{E}}&=\\frac{k_B T \\sqrt{Bt}}{k_B T B t}. \\\\\n",
    "\\frac{\\sigma_E}{\\overline{E}}&=\\frac{1}{\\sqrt{Bt}}. \n",
    "\\tag{7.5}\n",
    "\\end{align*}$$\n",
    "\n",
    "Let us interpret this result. Suppose we measure the energy incident from our resistive source at temperature $T$ over a time interval $t$. What result can we hope to get? It won't be exactly the average $\\overline{E}$, there will be some spread about this value. The root mean square, or standard deviation, of that spread is $\\sigma_E$. If we divide the energy received by the time interval over which it was received, $t$, then $\\overline{E}/t$ is an estimate of the noise power $P_N$ emitted by the source. The spread in the recevied value is $\\sigma_N=\\sigma_E/t$. Because both $\\overline{E}$ and $\\sigma_E$ have been divided by the same time interval $t$, their ratio remains the same, so we can write\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\sigma_N}{P_N}&=\\frac{1}{\\sqrt{Bt}} \\\\\n",
    "\\frac{1}{\\sigma_N}&=\\frac{1}{P_N}\\sqrt{Bt},\n",
    "\\tag{7.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "where on the last line I have rearranged the terms in a manner that will prove convenient in the ensuing discussion. The important application of this result to our work is in the use of power spectra to express the output of detectors in a series of frequency bands, each of bandwidth $B$, where the power spectrum is fashioned out of data taken over a time interval $t$. Each of these frequency bands, or bins, has an average power $P_N$. They are all the same because Johnson noise is approximately independent of frequency in the low frequency limit. However, what you see in a power spectrum is a jagged line about an average value, because of the fluctuations in the measured power about the average power. The root mean square of these fluctuations is $\\sigma_N$. It gets smaller if you either increase the amount of time over which the power was estimated, or you increase the bandwidth of each of the bins.\n",
    "\n",
    "Now, suppose one of the bins contains a small signal $P_S$ in addition to the noise. This signal is always present, and it is only present, for simplicity, in one of the bands. As you acquire power for longer and longer, the root mean square fluctuation between bins drops as the square root of the integration time. Put simply, the power spectrum gets smoother, and if you integrate for long enough, the signal becomes visible above the fluctuations in the noise. \n",
    "\n",
    "We define the signal to noise ratio, or $\\rm SNR$, as the ratio of the signal power $P_S$ to the root mean square noise fluctuation, $\\sigma_N$. Inserting $P_S$ into Equation 7.5, we obtain the form in which the radiometer equation is usually written for its applications to radio astronomy and axion searches.\n",
    "\n",
    "$$\\begin{align*}\n",
    "{\\rm SNR}=\\frac{P_S}{\\sigma_N}=\\frac{P_S}{P_N}\\sqrt{Bt}. \n",
    "\\tag{7.7}\n",
    "\\end{align*}$$\n",
    "\n",
    "### Application to axion detection\n",
    "\n",
    "Let us see why this is so useful for axion searches. Suppose your signal power over the bandwidth of the axion signal is a tenth that of the raw noise power. This means that $P_S/P_N=0.1$. Suppose the axion signal covers a bandwidth of $\\rm 1\\,kHz$. Let us say that you can make out the axion signal when it is four times the height of the root mean square noise fluctuations, so that the target $\\rm SNR$ is $4$. How long do you have to integrate to achieve this signal to noise ratio? Equation 7.7 tells us the answer. It becomes $4=0.1\\sqrt{1000t}$, so $1000t=40^2=1600$, therefore $t=1.6\\,{\\rm s}$. So in 1.6 seconds, even with a raw signal power that is only a tenth that of the raw noise power, you'll still be able to see the axion signal as a small power excess on top of the fluctuations in the surrounding noise floor. \n",
    "\n",
    "However, the win by averaging over long times for weak signals is a slow one because of the square root. Suppose instead $P_S/P_N$ were $0.01$. In this case you have $4=0.01\\sqrt{1000t}$, so that $1000t=1.6\\times10^5$, You need to integrate for $\\rm 160\\,s$ to achieve an $\\rm SNR$ of 4. If the integration times get too long, then aspects of your detector may start to drift, and this drift will spoil the effect of the averaging on the noise fluctuations by causing the baseline, or noise floor, to drift around."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
